<!doctype html>
<html>
  <head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1, shrink-to-fit=no"
    />
    <title>Stlite App</title>
    <link
      rel="stylesheet"
      href="https://cdn.jsdelivr.net/npm/@stlite/browser@0.76.0/build/style.css"
    />
  </head>
  <body>
    <div id="root"></div>
    <script type="module">
      import { mount } from "https://cdn.jsdelivr.net/npm/@stlite/browser@0.76.0/build/stlite.js";
      mount(
        {
            "requirements": ['pandas', 'fuzzywuzzy', 'openpyxl', 'https://plasmogenepi.github.io/pmotool-app-web/assets/pmotools_python-0.1.0-py3-none-any.whl'],
            "entrypoint": "PMO_Builder.py",
            "files": {
                
                "PMO_Builder.py": `import streamlit as st
from src.format_page import render_header


def main():
    render_header()
    # Introduction Section
    st.subheader("About the PMO Builder", divider="gray")
    st.markdown(
        """
        The **PMO Builder** is designed to help create and manage PMO 
        (Portable Microhaplotype Object) files from your own data to organize 
        and store information in a standardized format. This app simplifies the 
        conversion of your data from multiple CSV files into the relational PMO format.
        """
    )

    # Key Features
    st.subheader("Components", divider="gray")
    st.markdown(
        """
        As you move through the app you will put together the following information. Together these will make a complete PMO:
        - **Panel Information**: A table including data on the targets that make up the panel.
        - **Sequencing Information**: Information on how the samples were sequenced.
        - **Bioinformatics Information**: Information on the bioinformatics pipeline used to generate the allele data.
        - **Allele table**: A table containing the alleles called for each of the samples for each of the targets and the reads associated.
        - **Demultiplexed reads**: A table containing the raw reads for each sample, for each target after demultiplexing, before any filtering.
        - **Experimental metadata**: Information on the sequencing run, for example where each sample was located on the plate.
        - **Specimen Information**: metadata on the biological samples.

        More information on the file format can be found [here](https://plasmogenepi.github.io/PMO_Docs/format/FormatOverview.html)
        """
    )

    # How this will work
    st.subheader("How building your PMO will work", divider="gray")
    st.markdown(
        """
        For each of the components above you will...
        - **Upload Data**: Upload your version of the information.
        - **Map Fields**: Map your data fields to match the PMO format.
        - **Save Progress (Optional)**: If you may reuse the section (e.g. Panel Information) you can save it for future PMO file generation.

        Once you have all of the parts together you can merge the parts and export your completed PMO file in JSON format.
        """
    )

    # Call to Action
    st.markdown(
        """
            ---
            ### Ready to Get Started?
            Select **Panel Information** from the sidebar to begin building your PMO file!
            """
    )


if __name__ == "__main__":
    main()
`,
                
                "pages/2_Specimen_Level_Metadata.py": `import streamlit as st
from src.format_page import render_header
from src.data_loader import load_csv
from src.field_matcher import fuzzy_field_matching_page_section, interactive_field_mapping_page_section
from src.transformer import transform_specimen_info
from src.utils import load_schema


class SpecimenMetadataPage:
    def __init__(self, target_schema, alternate_schema_names):
        self.target_schema = target_schema
        self.alternate_schema_names = alternate_schema_names

    def upload_csv(self):
        st.subheader("Upload File")
        return st.file_uploader("Upload a TSV file", type=["csv", "tsv", "xlsx", "xls", "txt"])

    def field_mapping(self, df):
        # Fuzzy field matching
        field_mapping, unused_field_names = fuzzy_field_matching_page_section(
            df, self.target_schema, self.alternate_schema_names)

        # Interactive field mapping
        field_mapping = interactive_field_mapping_page_section(
            field_mapping, df.columns.tolist())
        return field_mapping, unused_field_names

    def add_additional_fields(self, unused_field_names):
        st.subheader("Add Additional Fields")
        selected_additional_fields = None
        if unused_field_names:
            optional_additional_fields = st.toggle("Add additional fields")
            if optional_additional_fields:
                checkbox_states = {}
                st.write("Select the extra columns you would like to include:")
                for item in unused_field_names:
                    checkbox_states[item] = st.checkbox(label=item)
                selected_additional_fields = [
                    key for key, value in checkbox_states.items() if value]
                st.write("You selected:", selected_additional_fields)
        return selected_additional_fields

    def transform_and_save_data(self, df, field_mapping, selected_additional_fields):
        st.subheader("Transform Data")
        if st.button("Transform Data"):
            transformed_df = transform_specimen_info(
                df, field_mapping, selected_additional_fields)
            st.session_state["specimen_info"] = transformed_df
            try:
                st.success(
                    f"Specimen Information has been saved!")
            except Exception as e:
                st.error(f"Error saving Microhaplotype Information: {e}")

    def run(self):
        # File upload
        uploaded_file = self.upload_csv()
        if uploaded_file:
            df = load_csv(uploaded_file)
            interactive_preview = st.toggle("Preview File")
            if interactive_preview:
                st.write("Uploaded File Preview:")
                st.dataframe(df)

            field_mapping, unused_field_names = self.field_mapping(df)

            # Add additional fields
            selected_additional_fields = self.add_additional_fields(
                unused_field_names)

            # Transform and save data
            self.transform_and_save_data(
                df, field_mapping, selected_additional_fields)


if __name__ == "__main__":
    render_header()
    st.subheader("Specimen Level Metadata Converter", divider="gray")
    schema_fields = load_schema()
    target_schema = schema_fields["specimen_level_metadata"]["required"]
    alternate_schema_names = schema_fields["specimen_level_metadata"]["alternatives"]
    app = SpecimenMetadataPage(target_schema, alternate_schema_names)
    app.run()
`,
                
                "pages/1_Panel_Information.py": `import streamlit as st
import json
import os
from src.data_loader import load_csv
from src.field_matcher import fuzzy_field_matching_page_section, interactive_field_mapping_page_section
from src.transformer import transform_panel_info
from src.format_page import render_header
from src.utils import load_schema


class PanelManager:
    def __init__(self, save_dir):
        self.save_dir = save_dir

    def check_save_dir(self):
        """Check dir exists or create it."""
        os.makedirs(self.save_dir, exist_ok=True)

    def save_panel(self, panel_name, panel_data):
        """Save panel data to a JSON file."""
        with open(os.path.join(self.save_dir, f"{panel_name}.json"), "w") as f:
            json.dump(panel_data, f)

    def load_panel(self, panel_name):
        """Load panel data from a JSON file."""
        with open(os.path.join(self.save_dir, f"{panel_name}.json"), "r") as f:
            return json.load(f)

    def get_saved_panels(self):
        """Get a list of saved panel names."""
        return [f.split(".json")[0] for f in os.listdir(self.save_dir) if f.endswith(".json")]


class PanelPage:
    def __init__(self, save_dir, target_schema, alternate_schema_names):
        self.save_dir = save_dir
        self.panel_manager = PanelManager(self.save_dir)
        self.panel_manager.check_save_dir()
        self.target_schema = target_schema
        self.alternate_schema_names = alternate_schema_names

    def load_saved_panel(self):
        use_past = st.checkbox("Use a past version")
        if use_past:
            saved_panels = self.panel_manager.get_saved_panels()
            if saved_panels:
                selected_panel = st.selectbox(
                    "Select a saved panel:", saved_panels)
                if st.button("Load Panel"):
                    panel_data = self.panel_manager.load_panel(selected_panel)
                    st.session_state["panel_info"] = panel_data
                    st.success(f"Loaded panel: {selected_panel}")
            else:
                st.warning("No saved panels found.")

    def panel_id_input(self):
        st.subheader("Panel ID")
        return st.text_input("Enter panel ID:", help='Identifier for the panel.')

    def upload_csv(self):
        st.subheader("Upload File")
        return st.file_uploader("Upload a TSV file", type=["csv", "tsv", "xlsx", "xls", "txt"])

    def field_mapping(self, df):
        # Fuzzy field matching
        field_mapping, unused_field_names = fuzzy_field_matching_page_section(
            df, self.target_schema, self.alternate_schema_names)

        # Interactive field mapping
        field_mapping = interactive_field_mapping_page_section(
            field_mapping, df.columns.tolist())
        return field_mapping, unused_field_names

    def add_additional_fields(self, unused_field_names):
        st.subheader("Add Additional Fields")
        selected_additional_fields = None
        if unused_field_names:
            optional_additional_fields = st.toggle(
                "Add additional fields from your table")
            if optional_additional_fields:
                checkbox_states = {}
                st.write("Select the extra columns you would like to include:")
                for item in unused_field_names:
                    checkbox_states[item] = st.checkbox(label=item)
                selected_additional_fields = [
                    key for key, value in checkbox_states.items() if value]
                st.write("You selected:", selected_additional_fields)
        return selected_additional_fields

    def add_genome_information(self):
        st.subheader("Add Genome Information")
        genome_name = st.text_input("Name:", help='Name of the genome.')
        taxon_id = st.text_input("Taxon ID:", help='The NCBI taxonomy number.')
        version = st.text_input("Version:", help='The genome version.')
        genome_url = st.text_input("URL:", help='A link to the genome file.')
        gff_url = st.text_input("GFF URL (Optional):",
                                help='A link to the genome’s annotation file')
        genome_info = {
            "name": genome_name,
            "taxon_id": taxon_id,
            "url": genome_url,
            "version": version,
        }
        if gff_url:
            genome_info["gff_url"] = gff_url
        return genome_info

    def transform_and_save_data(self, df, panel_ID, field_mapping, genome_info, selected_additional_fields):
        if all([panel_ID, genome_info["name"], genome_info["taxon_id"], genome_info["version"], genome_info["url"]]):
            st.subheader("Transform Data")
            if st.button("Transform Data"):
                transformed_df = transform_panel_info(
                    df, panel_ID, field_mapping, genome_info, selected_additional_fields)
                # json_data = json.dumps(transformed_df, indent=4)
                st.session_state["panel_info"] = transformed_df
                try:
                    self.panel_manager.save_panel(panel_ID, transformed_df)
                    st.success(f"Panel '{panel_ID}' has been saved!")
                except Exception as e:
                    st.error(f"Error saving panel: {e}")

    def display_panel_info(self):
        if "panel_info" in st.session_state:
            preview = st.toggle("Preview Panel Information")
            if preview:
                st.write("Current Panel Information:")
                st.json(st.session_state["panel_info"])

    def run(self):
        # Load past panel if applicable
        self.load_saved_panel()

        # Input for panel ID
        panel_ID = self.panel_id_input()

        # File upload
        uploaded_file = self.upload_csv()
        if uploaded_file:
            df = load_csv(uploaded_file)
            interactive_preview = st.toggle("Preview File")
            if interactive_preview:
                st.write("Uploaded File Preview:")
                st.dataframe(df)

            field_mapping, unused_field_names = self.field_mapping(df)

            # Add additional fields
            selected_additional_fields = self.add_additional_fields(
                unused_field_names)

            # Add genome information
            genome_info = self.add_genome_information()

            # Transform and save data
            self.transform_and_save_data(
                df, panel_ID, field_mapping, genome_info, selected_additional_fields)

        # Display current panel information
        self.display_panel_info()


# Initialize and run the app
if __name__ == "__main__":
    render_header()
    st.subheader("Panel Information Converter", divider="gray")
    schema_fields = load_schema()
    target_schema = schema_fields["panel_info"]["required"]
    alternate_schema_names = schema_fields["panel_info"]["alternatives"]
    app = PanelPage(os.path.join(os.getcwd(), "saved_panels"),
                    target_schema, alternate_schema_names)
    app.run()
`,
                
                "pages/6_Sequencing_Information.py": `import streamlit as st
from src.format_page import render_header


class SeqInfoPage:
    def __init__(self):
        self.seq_info = {}

    def add_required_seq_information(self):
        st.subheader("Add Sequencing Information")

        self.seq_info["sequencing_info_id"] = st.text_input(
            "Sequencing Information ID:", help='A unique identifier for this sequencing info.')
        seq_instrument = st.text_input(
            "Sequencing Instrument:", help='The sequencing instrument used to sequence the run, e.g. ILLUMINA, Illumina MiSeq.')
        seq_date = st.date_input(
            "Sequencing Date:", help='The date of sequencing, should be YYYY-MM or YYYY-MM-DD.')
        nucl_acid_ext = st.text_input(
            "Nucleic Acid Extraction:", help='Link to a reference or kit that describes the recovery of nucleic acids from the sample.')
        nucl_acid_amp = st.text_input(
            "Nucleic Acid Amplification:", help='Link to a reference or kit that describes the enzymatic amplification of nucleic acids.')
        nucl_acid_ext_date = st.date_input(
            "Nucleic Acid Extraction Date:", help='The date of the nucleoacide extraction.')
        nucl_acid_amp_date = st.date_input(
            "Nucleic Acid Amplification Date:", help='The date of the nucleoacide amplification.')
        pcr_cond = st.text_input(
            "PCR Conditions:", help='The method/conditions for PCR, List PCR cycles used to amplify the target.')
        lib_screen = st.text_input(
            "Library Screen:", help='Describe enrichment, screening, or normalization methods applied during amplification or library preparation, e.g. size selection 390bp, diluted to 1 ng DNA/sample.')
        lib_layout = st.text_input(
            "Library Layout:", help='Specify the configuration of reads, e.g. paired-end.')
        lib_kit = st.text_input(
            "Library Kit:", help='Name, version, and applicable cell or cycle numbers for the kit used to prepare libraries and load cells or chips for sequencing. If possible, include a part number, e.g. MiSeq Reagent Kit v3 (150-cycle), MS-102-3001.')
        seq_center = st.text_input(
            "Sequencing Center:", help='Name of facility where sequencing was performed (lab, core facility, or company).')

        # self.seq_info["sequencing_info_id"] = sequencing_info_id
        self.seq_info["seq_instrument"] = seq_instrument
        self.seq_info["seq_date"] = str(seq_date)
        self.seq_info["nucl_acid_ext"] = nucl_acid_ext
        self.seq_info["nucl_acid_amp"] = nucl_acid_amp
        self.seq_info["nucl_acid_ext_date"] = str(nucl_acid_ext_date)
        self.seq_info["nucl_acid_amp_date"] = str(nucl_acid_amp_date)
        self.seq_info["pcr_cond"] = pcr_cond
        self.seq_info["lib_screen"] = lib_screen
        self.seq_info["lib_layout"] = lib_layout
        self.seq_info["lib_kit"] = lib_kit
        self.seq_info["seq_center"] = seq_center

    def add_additional_fields(self):
        st.title("Add Additional Fields")

        # Add a toggle to enable additional fields
        add_fields_toggle = st.checkbox("Add Additional Fields")

        if add_fields_toggle:
            st.write("Fill in the additional fields below:")
            number_inputs = st.number_input("Number of additional inputs",
                                            min_value=0, value=1)
            # Inputs for names and values
            cols = st.columns(2)
            with cols[0]:
                field_names = [st.text_input(
                    f'Field Name {i}', key=f"field_name_{i}") for i in range(number_inputs)]
            with cols[1]:
                field_values = [st.text_input(
                    f'Value {i}', key=f"value_{i}") for i in range(number_inputs)]

            # Save the additional fields
            for i in range(number_inputs):
                self.seq_info[field_names[i]] = field_values[i]

    def transform_and_save_data(self):
        seq_info = self.seq_info
        if all([seq_info["sequencing_info_id"], seq_info["seq_instrument"], seq_info["seq_date"],
                seq_info["nucl_acid_ext"], seq_info["nucl_acid_amp"], seq_info["nucl_acid_ext_date"], seq_info["nucl_acid_amp_date"],
                seq_info["pcr_cond"], seq_info["lib_screen"], seq_info["lib_layout"], seq_info["lib_kit"],
                seq_info["seq_center"],]):
            st.subheader("Save Data")
            if st.button("Save Data"):
                st.session_state["seq_info"] = {
                    seq_info["sequencing_info_id"]: seq_info}

    def display_info(self):
        if "seq_info" in st.session_state:
            st.subheader("Preview Sequencing Information")
            preview_toggle = st.toggle("Preview Sequencing Information")
            if preview_toggle:
                st.write("Current Sequencing Information:")
                st.json(st.session_state["seq_info"])

    def run(self):
        self.add_required_seq_information()
        self.add_additional_fields()
        self.transform_and_save_data()
        self.display_info()


# Initialize and run the page
if __name__ == "__main__":
    render_header()
    st.subheader("Sequencing Information", divider="gray")
    app = SeqInfoPage()
    app.run()
`,
                
                "pages/5_Demultiplexed_Samples.py": `import streamlit as st
from src.data_loader import load_csv
from src.field_matcher import fuzzy_field_matching_page_section, interactive_field_mapping_page_section
from src.transformer import transform_demultiplexed_info
from src.format_page import render_header
from src.utils import load_schema


class DemultiplexPage:
    def __init__(self, target_schema, alternate_schema_names):
        self.target_schema = target_schema
        self.alternate_schema_names = alternate_schema_names

    def upload_csv(self):
        st.subheader("Upload File")
        return st.file_uploader("Upload a TSV file", type=["csv", "tsv", "xlsx", "xls", "txt"])

    def field_mapping(self, df):
        # Fuzzy field matching
        field_mapping, unused_field_names = fuzzy_field_matching_page_section(
            df, self.target_schema, self.alternate_schema_names)

        # Interactive field mapping
        field_mapping = interactive_field_mapping_page_section(
            field_mapping, df.columns.tolist())
        return field_mapping, unused_field_names

    def add_additional_fields(self, unused_field_names):
        st.subheader("Add Additional Fields")
        selected_additional_fields = None
        if unused_field_names:
            optional_additional_fields = st.toggle("Add additional fields")
            if optional_additional_fields:
                checkbox_states = {}
                st.write("Select the extra columns you would like to include:")
                for item in unused_field_names:
                    checkbox_states[item] = st.checkbox(label=item)
                selected_additional_fields = [
                    key for key, value in checkbox_states.items() if value]
                st.write("You selected:", selected_additional_fields)
        return selected_additional_fields

    def bioinfo_id_input(self):
        st.subheader("Bioinformatics ID")
        return st.text_input("Enter bioinfo ID:", help='Identifier for the bioinformatics run.')

    def transform_and_save_data(self, df, bioinfo_ID, field_mapping, selected_additional_fields):
        if bioinfo_ID:
            st.subheader("Transform Data")
            if st.button("Transform Data"):
                transformed_df = transform_demultiplexed_info(
                    df, bioinfo_ID, field_mapping, selected_additional_fields)
                st.session_state["demultiplexed_data"] = transformed_df
                try:
                    st.success(
                        f"Demultiplexed Information from Bioinformatics Run '{bioinfo_ID}' has been saved!")
                except Exception as e:
                    st.error(f"Error saving Microhaplotype Information: {e}")

    def run(self):
        # File upload
        uploaded_file = self.upload_csv()
        if uploaded_file:
            df = load_csv(uploaded_file)
            interactive_preview = st.toggle("Preview File")
            if interactive_preview:
                st.write("Uploaded File Preview:")
                st.dataframe(df)

            field_mapping, unused_field_names = self.field_mapping(df)

            # Add additional fields
            selected_additional_fields = self.add_additional_fields(
                unused_field_names)

            # Enter bioinformatics ID
            bioinfo_ID = self.bioinfo_id_input()

            self.transform_and_save_data(
                df, bioinfo_ID, field_mapping, selected_additional_fields)


# Initialize and run the app
if __name__ == "__main__":
    render_header()
    st.subheader("Microhaplotype Information Converter", divider="gray")
    schema_fields = load_schema()
    target_schema = schema_fields["demultiplexed_samples"]["required"]
    alternate_schema_names = schema_fields["demultiplexed_samples"]["alternatives"]
    app = DemultiplexPage(target_schema, alternate_schema_names)
    app.run()
`,
                
                "pages/3_Experiment_Level_Metadata.py": `import streamlit as st
from src.format_page import render_header
from src.data_loader import load_csv
from src.field_matcher import fuzzy_field_matching_page_section, interactive_field_mapping_page_section
from src.transformer import transform_experiment_info
from src.utils import load_schema


class ExperimentMetadataPage:
    def __init__(self, target_schema, alternate_schema_names):
        self.target_schema = target_schema
        self.alternate_schema_names = alternate_schema_names

    def upload_csv(self):
        st.subheader("Upload File")
        return st.file_uploader("Upload a TSV file", type=["csv", "tsv", "xlsx", "xls", "txt"])

    def field_mapping(self, df):
        # Fuzzy field matching
        field_mapping, unused_field_names = fuzzy_field_matching_page_section(
            df, self.target_schema, self.alternate_schema_names)

        # Interactive field mapping
        field_mapping = interactive_field_mapping_page_section(
            field_mapping, df.columns.tolist())
        return field_mapping, unused_field_names

    def add_additional_fields(self, unused_field_names):
        st.subheader("Add Additional Fields")
        selected_additional_fields = None
        if unused_field_names:
            optional_additional_fields = st.toggle("Add additional fields")
            if optional_additional_fields:
                checkbox_states = {}
                st.write("Select the extra columns you would like to include:")
                for item in unused_field_names:
                    checkbox_states[item] = st.checkbox(label=item)
                selected_additional_fields = [
                    key for key, value in checkbox_states.items() if value]
                st.write("You selected:", selected_additional_fields)
        return selected_additional_fields

    def transform_and_save_data(self, df, field_mapping, selected_additional_fields):
        st.subheader("Transform Data")
        if st.button("Transform Data"):
            transformed_df = transform_experiment_info(
                df, field_mapping, selected_additional_fields)
            st.session_state["experiment_info"] = transformed_df
            try:
                st.success(
                    f"Specimen Information has been saved!")
            except Exception as e:
                st.error(f"Error saving Microhaplotype Information: {e}")

    def run(self):
        # File upload
        uploaded_file = self.upload_csv()
        if uploaded_file:
            df = load_csv(uploaded_file)
            interactive_preview = st.toggle("Preview File")
            if interactive_preview:
                st.write("Uploaded File Preview:")
                st.dataframe(df)

            field_mapping, unused_field_names = self.field_mapping(df)

            # Add additional fields
            selected_additional_fields = self.add_additional_fields(
                unused_field_names)

            # Transform and save data
            self.transform_and_save_data(
                df, field_mapping, selected_additional_fields)


if __name__ == "__main__":
    render_header()
    st.subheader("Experiment Level Metadata Converter", divider="gray")
    schema_fields = load_schema()
    target_schema = schema_fields["experiment_level_metadata"]["required"]
    alternate_schema_names = schema_fields["experiment_level_metadata"]["alternatives"]
    app = ExperimentMetadataPage(target_schema, alternate_schema_names)
    app.run()
`,
                
                "pages/7_Bioinformatics_Information.py": `import streamlit as st
from src.format_page import render_header


class BioInfoPage:
    def __init__(self):
        self.bioinfo_infos = {}

    def enter_bioinfo_method_vals(self, method):
        cols = st.columns(4)
        with cols[0]:
            program = st.text_input(f'program', key=f"{method}_program")
        with cols[1]:
            purpose = st.text_input(f'purpose', key=f"{method}_purpose")
        with cols[2]:
            version = st.text_input(f'version', key=f"{method}_version")
        with cols[3]:
            additional_argument = st.text_input(
                f'additional argument (optional)', key=f"{method}_additional_argument")
        method_dict = {"program": program,
                       "purpose": purpose,
                       "version": version}
        if additional_argument:
            method_dict["additional_argument"] = additional_argument
        # Add user specified additional fields
        method_dict = self.add_additional_fields(method, method_dict)
        return method_dict

    def add_additional_fields(self, method, dict):
        additional_fields_toggle = st.toggle(
            f"Add additional fields to {method}", key=f"{method}_toggle")
        if additional_fields_toggle:
            st.write("Fill in the additional fields below:")

            number_inputs = st.number_input("Number of additional inputs",
                                            min_value=0, value=1, key=f"{method}_num_fields")
            # Inputs for names and values
            cols = st.columns(2)
            with cols[0]:
                field_names = [st.text_input(
                    f'Field Name {i}', key=f"field_name_{method}_{i}") for i in range(number_inputs)]
            with cols[1]:
                field_values = [st.text_input(
                    f'Value {i}', key=f"value_{method}_{i}") for i in range(number_inputs)]

            # Save the additional fields
            for i in range(number_inputs):
                dict[field_names[i]] = field_values[i]
        return dict

    def check_method_required_fields(self, dict, fields=["program", "purpose", "version"]):
        for field in fields:
            if field not in dict.keys():
                return False
        return True

    def add_bioinfo_information(self):
        st.subheader("Add Bioinformatics Information")

        bioinfo_info_id = st.text_input(
            "Bioinformatics Information ID:", help='A unique identifier for this targeted amplicon bioinformatics pipeline run.')

        # Required Methods
        st.subheader("Demultiplexing Method")
        demultiplexing_method_dict = {
            "demultiplexing_method": self.enter_bioinfo_method_vals("demultiplexing_method")}
        st.subheader("Denoising Method")
        denoising_method_dict = {
            "denoising_method": self.enter_bioinfo_method_vals("denoising_method")}
        # Put basic info into dict
        self.bioinfo_infos = {
            "demultiplexing_method": demultiplexing_method_dict,
            "denoising_method": denoising_method_dict,
            "tar_amp_bioinformatics_info_id": bioinfo_info_id
        }

        # # Optional
        # st.subheader("Add Optional Methods")
        # pop_clust_method_dict = {}
        # pop_clust_box = st.checkbox(label="Population Clustering Method")
        # if pop_clust_box:
        #     pop_clust_method_dict["pop_clust_method"] = self.enter_bioinfo_method_vals(
        #         "pop_clust_method")
        # # if self.check_method_required_fields(pop_clust_method_dict):
        # self.bioinfo_infos["pop_clust_method"] = pop_clust_method_dict

        # # Add additional methods
        # st.subheader("Additional Methods")
        # additional_methods = st.toggle("Add additional Method Definitions")
        # if additional_methods:
        #     st.write(
        #         "Fill in the additional methods you would like to add the details of:")

        #     number_extra_methods = st.number_input(
        #         "Number of additional inputs", min_value=0, value=1, key=f"number_extra_methods")
        #     additional_methods = [st.text_input(
        #         f'Method Name {i}', key=f"method_name_{i}") for i in range(number_extra_methods)]

        #     for method in additional_methods:
        #         if method:
        #             st.subheader(method)
        #             new_method_dict = self.enter_bioinfo_method_vals(method)
        #             self.bioinfo_infos[method] = new_method_dict

    def display_info(self):
        if "bioinfo_infos" in st.session_state:
            st.subheader("Preview Bioinformatics Information")
            preview_toggle = st.toggle("Preview Bioinformatics Information")
            if preview_toggle:
                st.write("Current Bioinformatics Information:")
                st.json(st.session_state["bioinfo_infos"])

    def transform_and_save_data(self):
        bioinfo_infos = self.bioinfo_infos
        if all([bioinfo_infos["demultiplexing_method"], bioinfo_infos["denoising_method"], bioinfo_infos["tar_amp_bioinformatics_info_id"],]):
            st.subheader("Save Data")
            if st.button("Save Data"):
                st.session_state["bioinfo_infos"] = bioinfo_infos

    def run(self):
        # Add bioinformatics information
        self.add_bioinfo_information()
        self.transform_and_save_data()
        self.display_info()


# Initialize and run the page
if __name__ == "__main__":
    render_header()
    st.subheader("Bioinformatics Information", divider="gray")
    app = BioInfoPage()
    app.run()
`,
                
                "pages/8_Create_Final_PMO.py": `import streamlit as st
import json
import os
from src.format_page import render_header

current_directory = os.getcwd()  # Get the current working directory
SAVE_DIR = os.path.join(current_directory, "PMO")
os.makedirs(SAVE_DIR, exist_ok=True)

render_header()
st.subheader("Create Final PMO", divider="gray")

st.subheader("Components")
# PANEL INFO
if "panel_info" in st.session_state:
    panel_info = st.session_state["panel_info"]
    panel_id = ', '.join(panel_info["panel_info"].keys())
    st.write("**Current Panel Information:**", panel_id)
else:
    st.error(
        "No panel information found. Please go to the Panel Information tab before proceeding.")

# SPECIMEN INFO
if "specimen_info" in st.session_state:
    spec_samples = ', '.join(
        list(st.session_state["specimen_info"].keys())[:10])
    st.write("**Current Specimen Information for samples (showing first 10):** ",
             spec_samples, '...')
else:
    st.error(
        "No specimen information found. Please go to the Specimen Information tab before proceeding.")

# EXPERIMENT INFO
if "experiment_info" in st.session_state:
    experiment_samples = ', '.join(
        list(st.session_state["experiment_info"].keys())[:10])
    st.write("**Current Experiment Information for samples (showing first 10):** ",
             experiment_samples, '...')
else:
    st.error(
        "No experiment information found. Please go to the Experiment Information tab before proceeding.")

# MICROHAPLOTYPE INFO
if "mhap_data" in st.session_state:
    bioinfo_id = ', '.join(st.session_state["mhap_data"]["microhaplotypes_detected"].keys()
                           )
    st.write(
        "**Current Microhaplotype Information from bioinformatics run:**", bioinfo_id)
else:
    st.error(
        "No microhaplotype information found. Please go to the Microhaplotype Information tab before proceeding.")

# DEMULTIPLEXED INFO
if "demultiplexed_data" in st.session_state:
    # demultiplexed_data = st.session_state["demultiplexed_data"]
    demultiplexed_bioinfo_id = st.session_state["demultiplexed_data"].keys()
    st.write("**Current Demultiplexed Information from bioinformatics run:**",
             ', '.join(demultiplexed_bioinfo_id))
else:
    st.error(
        "No demultiplexed information found. Please go to the Demultiplexed Samples tab before proceeding.")

# SEQUENCING INFO
if "seq_info" in st.session_state:
    seq_info_id = st.session_state["seq_info"].keys(
    )
    st.write("**Current Sequencing Information had ID:**",
             ', '.join(seq_info_id))
else:
    st.error(
        "No sequencing information found. Please go to the Sequencing Information tab before proceeding.")

# BIOINFORMATICS INFO
if "bioinfo_infos" in st.session_state:
    bioinfo_info_id = st.session_state["bioinfo_infos"]["tar_amp_bioinformatics_info_id"]
    st.write("**Current Bioinformatics Information had ID:**", bioinfo_info_id)
else:
    st.error(
        "No bioinformatics information found. Please go to the Bioinformatics Information tab before proceeding.")

# MERGE DATA
st.subheader("Merge Components to Final PMO")
if st.button("Merge Data"):
    formatted_pmo = {
        "experiment_infos": st.session_state["experiment_info"],
        "sequencing_infos": st.session_state["seq_info"],
        "specimen_infos": st.session_state["specimen_info"],
        "taramp_bioinformatics_infos": st.session_state["bioinfo_infos"],
        **st.session_state["mhap_data"],
        **panel_info,
        **st.session_state["demultiplexed_data"],
    }
    output_path = os.path.join(SAVE_DIR, f"{panel_id}_{bioinfo_id}.json")
    with open(output_path, "w") as f:
        json.dump(formatted_pmo, f, indent=4)
    st.success(f"Your PMO has been saved! It can be found here: {output_path}")
`,
                
                "pages/4_Microhaplotype_Information.py": `import streamlit as st
from src.data_loader import load_csv
from src.field_matcher import fuzzy_field_matching_page_section, interactive_field_mapping_page_section
from src.transformer import transform_mhap_info
from src.format_page import render_header
from src.utils import load_schema


class MhapPage:
    def __init__(self, target_schema, alternate_schema_names):
        self.target_schema = target_schema
        self.alternate_schema_names = alternate_schema_names

    def upload_csv(self):
        st.subheader("Upload File")
        return st.file_uploader("Upload a TSV file", type=["csv", "tsv", "xlsx", "xls", "txt"])

    def field_mapping(self, df):
        # Fuzzy field matching
        field_mapping, unused_field_names = fuzzy_field_matching_page_section(
            df, self.target_schema, self.alternate_schema_names)

        # Interactive field mapping
        field_mapping = interactive_field_mapping_page_section(
            field_mapping, df.columns.tolist())
        return field_mapping, unused_field_names

    def add_additional_fields(self, unused_field_names):
        st.subheader("Add Additional Fields")
        selected_additional_fields = None
        if unused_field_names:
            optional_additional_fields = st.toggle("Add additional fields")
            if optional_additional_fields:
                checkbox_states = {}
                st.write("Select the extra columns you would like to include:")
                for item in unused_field_names:
                    checkbox_states[item] = st.checkbox(label=item)
                selected_additional_fields = [
                    key for key, value in checkbox_states.items() if value]
                st.write("You selected:", selected_additional_fields)
        return selected_additional_fields

    def bioinfo_id_input(self):
        st.subheader("Bioinformatics ID")
        return st.text_input("Enter bioinfo ID:", help='Identifier for the bioinformatics run.')

    def transform_and_save_data(self, df, bioinfo_ID, field_mapping, selected_additional_fields):
        if bioinfo_ID:
            st.subheader("Transform Data")
            if st.button("Transform Data"):
                transformed_df = transform_mhap_info(
                    df, bioinfo_ID, field_mapping, selected_additional_fields)
                # json_data = json.dumps(transformed_df, indent=4)
                st.session_state["mhap_data"] = transformed_df
                try:
                    st.success(
                        f"Microhaplotype Information from Bioinformatics Run '{bioinfo_ID}' has been saved!")
                except Exception as e:
                    st.error(f"Error saving Microhaplotype Information: {e}")

    def run(self):
        # File upload
        uploaded_file = self.upload_csv()
        if uploaded_file:
            df = load_csv(uploaded_file)
            interactive_preview = st.toggle("Preview File")
            if interactive_preview:
                st.write("Uploaded File Preview:")
                st.dataframe(df)

            field_mapping, unused_field_names = self.field_mapping(df)

            # Add additional fields
            selected_additional_fields = self.add_additional_fields(
                unused_field_names)

            # Enter bioinformatics ID
            bioinfo_ID = self.bioinfo_id_input()

            self.transform_and_save_data(
                df, bioinfo_ID, field_mapping, selected_additional_fields)


# Initialize and run the app
if __name__ == "__main__":
    render_header()
    st.subheader("Microhaplotype Information Converter", divider="gray")
    schema_fields = load_schema()
    target_schema = schema_fields["mhap_info"]["required"]
    alternate_schema_names = schema_fields["mhap_info"]["alternatives"]
    app = MhapPage(target_schema, alternate_schema_names)
    app.run()
`,
                
                "src/field_matcher.py": `from fuzzywuzzy import process
from collections import Counter
import pandas as pd
import streamlit as st


def fuzzy_match_fields(field_names, target_schema, alternate_schema_names=None):
    """
    Matches field names to the target schema using fuzzy matching, ensuring 
    that each target schema field is only matched to one field name.

    Args:
        field_names (list): List of column names to be matched.
        target_schema (list): List of standard schema fields to match against.

    Returns:
        dict: A dictionary mapping each field name to the best-matched schema field.
        list: A list of unused field names that could not be matched.
    """
    matches = {}
    unused_field_names = []  # To store any unused field names

    # For every target find the best matching field
    # TODO: Think about whether to allow multiple matches from a single field.
    for target in target_schema:
        best_match = process.extractOne(target, field_names)
        if alternate_schema_names:
            alt_targets = alternate_schema_names[target]
            for alt_target in alt_targets:
                alt_match = process.extractOne(alt_target, field_names)
                if alt_match[1] > best_match[1]:
                    best_match = alt_match
        best_match_field = best_match[0]
        matches[target] = best_match_field
    # Find unused field names
    unused_field_names = [f for f in field_names if f not in matches]
    return matches, unused_field_names


def check_for_duplicates(field_mapping):
    """
    Checks if there are any duplicate values in the dictionary.

    Args:
        field_mapping (dict): A dictionary mapping input field names to target schema fields.

    Returns:
        bool: True if no duplicates are found. Raises a ValueError if duplicates exist.
    """
    # Extract the values (target schema fields) from the dictionary
    counts = Counter(list(field_mapping.values()))
    duplicates = {item for item, count in counts.items() if count > 1}
    # Check for duplicates by comparing the length of the list to the length of the set
    if duplicates:
        raise ValueError(
            f"Duplicate target schema fields found: {duplicates}")

    return True


def interactive_field_mapping(field_mapping, df_columns):
    updated_mapping = {}

    for field, suggested_match in field_mapping.items():
        # Use streamlit widgets to allow the user to select a match from df columns
        if isinstance(suggested_match, list):  # For multiple possible matches
            updated_mapping[field] = st.selectbox(
                f"Select match for {field}",
                options=df_columns,
                index=df_columns.index(
                    suggested_match[0]) if suggested_match else 0
            )
        else:
            updated_mapping[field] = st.selectbox(
                f"Modify match for {field}",
                options=df_columns,
                index=df_columns.index(
                    suggested_match) if suggested_match else 0
            )

    return updated_mapping


def field_mapping_json_to_table(mapping):
    data = [{"PMO Field": key, "Input Field": value}
            for key, value in mapping.items()]
    df = pd.DataFrame(data)
    return df


def fuzzy_field_matching_page_section(df, target_schema, alternate_schema_names=None):
    st.subheader("Match Fields")
    field_mapping, unused_field_names = fuzzy_match_fields(
        df.columns.tolist(), target_schema, alternate_schema_names
    )
    st.write("Suggested Field Mapping:")
    st.dataframe(field_mapping_json_to_table(field_mapping))
    return field_mapping, unused_field_names


def interactive_field_mapping_page_section(field_mapping, df_columns):
    interactive_field_mapping_on = st.toggle(
        "Manually Alter Field Mapping")
    if interactive_field_mapping_on:
        updated_mapping = interactive_field_mapping(
            field_mapping, df_columns)
        st.write("Updated Field Mapping:")
        st.dataframe(field_mapping_json_to_table(updated_mapping))
        check_for_duplicates(updated_mapping)
        return updated_mapping
    return field_mapping
`,
                
                "src/format_page.py": `# utils.py
from src.field_matcher import fuzzy_field_matching_page_section, interactive_field_mapping_page_section
from src.data_loader import load_csv
import streamlit as st


def render_header():
    """
    Render a header with a logo alongside text.
    """
    st.set_page_config(
        page_title="PMO Builder",
        page_icon="📂",
        layout="wide",
    )

    # Create two columns for layout: logo + text
    col1, col2 = st.columns([1, 4])

    with col1:
        st.image(
            "images/PGE_logo.png"
        )

    with col2:
        # Add title and subtitle
        st.title("PMO File Builder")
        st.markdown("**Streamlined Workflow for Generating PMO Files**")


class BasicPage:
    def __init__(self, target_schema):
        self.target_schema = target_schema

    def upload_csv(self):
        st.subheader("Upload File")
        return st.file_uploader("Upload a TSV file", type="csv")

    def field_mapping(self, df):
        # Fuzzy field matching
        field_mapping, unused_field_names = fuzzy_field_matching_page_section(
            df, self.target_schema)

        # Interactive field mapping
        field_mapping = interactive_field_mapping_page_section(
            field_mapping, df.columns.tolist())
        return field_mapping, unused_field_names

    def add_additional_fields(self, unused_field_names):
        st.subheader("Add Additional Fields")
        selected_additional_fields = None
        if unused_field_names:
            optional_additional_fields = st.toggle("Add additional fields")
            if optional_additional_fields:
                checkbox_states = {}
                st.write("Select the extra columns you would like to include:")
                for item in unused_field_names:
                    checkbox_states[item] = st.checkbox(label=item)
                selected_additional_fields = [
                    key for key, value in checkbox_states.items() if value]
                st.write("You selected:", selected_additional_fields)
        return selected_additional_fields

    def run(self):
        # File upload
        uploaded_file = self.upload_csv()
        if uploaded_file:
            df = load_csv(uploaded_file)
            interactive_preview = st.toggle("Preview File")
            if interactive_preview:
                st.write("Uploaded File Preview:")
                st.dataframe(df)

            field_mapping, unused_field_names = self.field_mapping(df)

            # Add additional fields
            selected_additional_fields = self.add_additional_fields(
                unused_field_names)

            # TODO: Implement Transform and save data
`,
                
                "src/data_loader.py": `import pandas as pd


def load_csv(file):
    """Load a CSV file into a pandas DataFrame."""
    try:
        if file.name.endswith((".csv", ".tsv", ".txt")):
            df = pd.read_csv(file, sep="\t")
        elif file.name.endswith((".xlsx", ".xls")):
            df = pd.read_excel(file)
        else:
            raise ValueError(
                "Unsupported file format. Please upload a CSV, TSV, or Excel file.")
        return df

    except Exception as e:
        raise ValueError(f"Failed to read CSV: {e}")
`,
                
                "src/transformer.py": `from pmotools.json_convertors.microhaplotype_table_to_pmo_dict import microhaplotype_table_to_pmo_dict
from pmotools.json_convertors.panel_information_to_pmo_dict import panel_info_table_to_pmo_dict
from pmotools.json_convertors.metatable_to_json_meta import experiment_info_table_to_json, specimen_info_table_to_json
from pmotools.json_convertors.demultiplexed_targets_to_pmo_dict import demultiplexed_targets_to_pmo_dict


def transform_mhap_info(df, bioinfo_id, field_mapping, additional_hap_detected_cols=None):
    """Reformat the DataFrame based on the provided field mapping."""
    transformed_df = microhaplotype_table_to_pmo_dict(
        df, bioinfo_id, sampleID_col=field_mapping["sampleID"], locus_col=field_mapping['target_id'], mhap_col=field_mapping['asv'], reads_col=field_mapping['reads'], additional_hap_detected_cols=additional_hap_detected_cols)
    return transformed_df


def transform_panel_info(df, panel_id, field_mapping, target_genome_info, additional_target_info_cols=None):
    """Reformat the DataFrame based on the provided field mapping."""
    transformed_df = panel_info_table_to_pmo_dict(
        df,
        panel_id,
        target_genome_info,
        target_id_col=field_mapping["target_id"],
        forward_primers_seq_col=field_mapping["forward_primers"],
        reverse_primers_seq_col=field_mapping["reverse_primers"],
        additional_target_info_cols=additional_target_info_cols)
    return transformed_df


def transform_specimen_info(df, field_mapping, additional_fields=None):
    transformed_df = specimen_info_table_to_json(
        df,
        specimen_id_col=field_mapping["specimen_id"],
        samp_taxon_id=field_mapping["samp_taxon_id"],
        collection_date=field_mapping["collection_date"],
        collection_country=field_mapping["collection_country"],
        collector=field_mapping["collector"],
        samp_store_loc=field_mapping["samp_store_loc"],
        samp_collect_device=field_mapping["samp_collect_device"],
        project_name=field_mapping["project_name"],
        additional_specimen_cols=additional_fields
    )
    return transformed_df


def transform_experiment_info(df, field_mapping, additional_fields=None):
    transformed_df = experiment_info_table_to_json(
        df,
        experiment_sample_id_col=field_mapping["experiment_sample_id"],
        sequencing_info_id=field_mapping["sequencing_info_id"],
        specimen_id=field_mapping["specimen_id"],
        panel_id=field_mapping["panel_id"],
        additional_experiment_cols=additional_fields
    )
    return transformed_df


def transform_demultiplexed_info(df, bioinfo_id, field_mapping, additional_hap_detected_cols=None):
    """Reformat the DataFrame based on the provided field mapping."""
    transformed_df = demultiplexed_targets_to_pmo_dict(
        df, bioinfo_id, sampleID_col=field_mapping["sampleID"], target_id_col=field_mapping['target_id'], read_count_col=field_mapping['raw_read_count'], additional_hap_detected_cols=additional_hap_detected_cols)
    return transformed_df
`,
                
                "src/utils.py": `import json


def save_to_csv(df, output_path):
    """Save a DataFrame to a CSV file."""
    df.to_csv(output_path, index=False)
    return output_path


def load_schema():
    # Load schema field names from JSON
    with open('conf/schema.json', 'r') as file:
        schema_fields = json.load(file)
    return schema_fields
`,
                
                "src/__init__.py": ``,
                
                "images/PGE_logo.png": {'url': '/images/PGE_logo.png'},
                
                "conf/schema.json": `{
    "panel_info": {
        "required": [
            "target_id",
            "forward_primers",
            "reverse_primers"
        ],
        "alternatives": {
            "target_id": [
                "locus",
                "amplicon"
            ],
            "forward_primers": [
                "fwd_primers"
            ],
            "reverse_primers": [
                "rev_primers"
            ]
        }
    },
    "specimen_level_metadata": {
        "required": [
            "specimen_id",
            "samp_taxon_id",
            "collection_date",
            "collection_country",
            "collector",
            "samp_store_loc",
            "samp_collect_device",
            "project_name"
        ],
        "alternatives": {
            "specimen_id": [
                "sample_id",
                "bio_id",
                "specimen_name",
                "sample_accession"
            ],
            "samp_taxon_id": [
                "taxon_id",
                "species_id",
                "organism_id"
            ],
            "collection_date": [
                "date_of_collection",
                "sampling_date",
                "gathering_date"
            ],
            "collection_country": [
                "country_of_collection",
                "sampling_country",
                "origin_country",
                "country",
                "admin_level_0",
                "geo_admin0"
            ],
            "collector": [
                "sample_collector",
                "field_researcher",
                "gatherer"
            ],
            "samp_store_loc": [
                "site_location",
                "storage_location",
                "sample_storage_site"
            ],
            "samp_collect_device": [
                "collection_device",
                "sampling_tool",
                "gathering_equipment"
            ],
            "project_name": [
                "study_name",
                "research_project",
                "experiment_name"
            ]
        }
    },
    "experiment_level_metadata": {
        "required": [
            "sequencing_info_id",
            "specimen_id",
            "panel_id",
            "experiment_sample_id"
        ],
        "alternatives": {
            "sequencing_info_id": [
                "seq_info_id",
                "sequencing_id",
                "sequence_record_id",
                "sequencing_run",
                "seq_run"
            ],
            "specimen_id": [
                "sample_id",
                "bio_id",
                "specimen_code",
                "sample_accession"
            ],
            "panel_id": [
                "test_panel_id",
                "assay_id",
                "panel",
                "panel_name"
            ],
            "experiment_sample_id": [
                "exp_sample_id",
                "test_sample_id",
                "lab_sample_id",
                "sequencing_id",
                "extraction_id",
                "run_accession"
            ]
        }
    },
    "mhap_info": {
        "required": [
            "sampleID",
            "target_id",
            "asv",
            "reads"
        ],
        "alternatives": {
            "sampleID": [
                "specimen_id",
                "sample_id",
                "biosample_id"
            ],
            "target_id": [
                "locus",
                "marker_id",
                "amplicon",
                "locus_id"
            ],
            "asv": [
                "amplicon_sequence_variant",
                "otu",
                "sequence_variant",
                "microhaplotype",
                "allele",
                "seq",
                "sequence"
            ],
            "reads": [
                "read_count",
                "sequence_reads",
                "read_depth"
            ]
        }
    },
    "demultiplexed_samples": {
        "required": [
            "sampleID",
            "target_id",
            "raw_read_count"
        ],
        "alternatives": {
            "sampleID": [
                "specimen_id",
                "sample_id",
                "biosample_id"
            ],
            "target_id": [
                "locus",
                "marker_id",
                "amplicon",
                "locus_id"
            ],
            "raw_read_count": [
                "read_count",
                "sequence_reads",
                "read_depth",
                "demuliplexed",
                "demuliplexed_read_count"
            ]
        }
    },
    "sequencing_info": {
        "required": [
            "seq_date",
            "nucl_acid_ext",
            "nucl_acid_amp",
            "nucl_acid_ext_date",
            "nucl_acid_amp_date",
            "pcr_cond",
            "lib_screen",
            "lib_layout",
            "lib_kit",
            "seq_center"
        ]
    },
    "bioinformatics_info": {
        "required": [
            "demultiplexing_method",
            "denoising_method",
            "tar_amp_bioinformatics_info_id"
        ]
    }
}`,
                
            }
        },
        document.getElementById("root"),
      );
    </script>
  </body>
</html>